{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMLLAVMIxAYgtkOjCQ/6A5X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3193a823d08b4cc2890ea03c634cb802": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0874701c8416474e8560409a76d7bf17",
              "IPY_MODEL_67e6c51c59c243c9a54056f6ede89e72",
              "IPY_MODEL_969f11db9417487f837f5da08868fe2d"
            ],
            "layout": "IPY_MODEL_6511f026777b407da3c6e497ef591230"
          }
        },
        "0874701c8416474e8560409a76d7bf17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31fcb8eb7d2a4700a6bfbf123e9c807a",
            "placeholder": "​",
            "style": "IPY_MODEL_7e317d328b964320ae2a95baab4d1b27",
            "value": ""
          }
        },
        "67e6c51c59c243c9a54056f6ede89e72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39b5f146a3354d72a1ff656fa7f02226",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1b415186d93a41278c22990c4e5b7550",
            "value": 3
          }
        },
        "969f11db9417487f837f5da08868fe2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c2fa458b1754908adfdd025748627d5",
            "placeholder": "​",
            "style": "IPY_MODEL_a6ae14fbd85c4779b4b3bd1b6687f473",
            "value": "Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:17&lt;00:00,  5.64s/it]\n"
          }
        },
        "6511f026777b407da3c6e497ef591230": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31fcb8eb7d2a4700a6bfbf123e9c807a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e317d328b964320ae2a95baab4d1b27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "39b5f146a3354d72a1ff656fa7f02226": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b415186d93a41278c22990c4e5b7550": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3c2fa458b1754908adfdd025748627d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6ae14fbd85c4779b4b3bd1b6687f473": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndreYang333/LLMdemo/blob/main/Qwen7bRAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1z9hgGonDfU"
      },
      "outputs": [],
      "source": [
        "# 安装必要的库\n",
        "!pip install langchain vllm gptcache modelscope\n",
        "!pip install transformers==4.32.0 accelerate tiktoken einops scipy transformers_stream_generator==0.0.4 peft deepspeed\n",
        "!pip install auto-gptq optimum\n",
        "!pip install --upgrade transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from vllm import LLM, SamplingParams\n",
        "import time\n",
        "import os\n",
        "#使用modelscope,如果不设置该环境变量，将会从huggingface下载,中国用\n",
        "# os.environ['VLLM_USE_MODELSCOPE']='True'"
      ],
      "metadata": {
        "id": "eXl4BRY7naKc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZY-mw2pqwOp",
        "outputId": "592aa2e3-a158-4451-e1ba-762b4a340af2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Oct  4 18:22:30 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   54C    P8              12W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#无量化,最低显存占用约16.5GB\n",
        "#llm = LLM(model=\"qwen/Qwen-7B-Chat\", trust_remote_code=True)\n",
        "#int4量化,最低显存占用约7GB\n",
        "llm = LLM(model=\"qwen/Qwen-7B-Chat-int4\", trust_remote_code=True,gpu_memory_utilization=0.55,max_model_len=1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538,
          "referenced_widgets": [
            "3193a823d08b4cc2890ea03c634cb802",
            "0874701c8416474e8560409a76d7bf17",
            "67e6c51c59c243c9a54056f6ede89e72",
            "969f11db9417487f837f5da08868fe2d",
            "6511f026777b407da3c6e497ef591230",
            "31fcb8eb7d2a4700a6bfbf123e9c807a",
            "7e317d328b964320ae2a95baab4d1b27",
            "39b5f146a3354d72a1ff656fa7f02226",
            "1b415186d93a41278c22990c4e5b7550",
            "3c2fa458b1754908adfdd025748627d5",
            "a6ae14fbd85c4779b4b3bd1b6687f473"
          ]
        },
        "id": "9GN4gU3zncQk",
        "outputId": "f2332953-c840-469b-8c17-b5c85798ede7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 10-04 18:22:34 config.py:1652] Downcasting torch.float32 to torch.float16.\n",
            "WARNING 10-04 18:22:34 config.py:319] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
            "INFO 10-04 18:22:34 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='qwen/Qwen-7B-Chat-int4', speculative_config=None, tokenizer='qwen/Qwen-7B-Chat-int4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=1000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=qwen/Qwen-7B-Chat-int4, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 10-04 18:22:37 tokenizer.py:156] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\n",
            "INFO 10-04 18:22:38 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
            "INFO 10-04 18:22:38 selector.py:116] Using XFormers backend.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
            "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
            "/usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
            "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 10-04 18:22:40 model_runner.py:1014] Starting to load model qwen/Qwen-7B-Chat-int4...\n",
            "INFO 10-04 18:22:41 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
            "INFO 10-04 18:22:41 selector.py:116] Using XFormers backend.\n",
            "INFO 10-04 18:22:42 weight_utils.py:242] Using model weights format ['*.safetensors']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3193a823d08b4cc2890ea03c634cb802"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 10-04 18:23:02 model_runner.py:1025] Loading model weights took 5.5097 GB\n",
            "INFO 10-04 18:23:05 gpu_executor.py:122] # GPU blocks: 250, # CPU blocks: 512\n",
            "INFO 10-04 18:23:08 model_runner.py:1329] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
            "INFO 10-04 18:23:08 model_runner.py:1333] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
            "INFO 10-04 18:23:48 model_runner.py:1456] Graph capturing finished in 40 secs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = [\n",
        "'''\n",
        "Andre是不是大帅哥\n",
        "'''\n",
        "]\n",
        "\n",
        "sampling_params = SamplingParams(temperature=0.8,top_k=10, top_p=0.95,max_tokens=256,stop=[\"<|endoftext|>\",\"<|im_end|>\"])\n",
        "start_time = time.time()\n",
        "outputs = llm.generate(prompts, sampling_params)\n",
        "end_time = time.time()\n",
        "latency = end_time - start_time\n",
        "print(f\"Latency: {latency} seconds\")\n",
        "# Print the outputs.\n",
        "for output in outputs:\n",
        "    prompt = output.prompt\n",
        "    generated_text = output.outputs[0].text\n",
        "    print(f\"Prompt: {prompt} \\nGenerated text: \\n{generated_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a618tUuEo20v",
        "outputId": "a126d0b6-ca9d-476b-dd48-ab544ebbd36a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.48s/it, est. speed input: 1.10 toks/s, output: 44.00 toks/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Latency: 5.489413738250732 seconds\n",
            "Prompt: \n",
            "Andre是不是大帅哥\n",
            " \n",
            "Generated text: \n",
            "这个问题很难回答，因为审美是主观的，不同的人对于“大帅哥”的定义和标准也会有所不同。以下是一些可能的因素，可以帮助我们更好地回答这个问题：\n",
            "\n",
            "1. 面容特征：Andre的脸部特征包括他的眼睛、鼻子、嘴、下巴等。如果他的这些特征被许多人认为是吸引人的，那么他可能就是大帅哥。\n",
            "\n",
            "2. 体形：除了脸部特征之外，Andre的体形也是一个重要的因素。如果他的身材健硕，肌肉发达，那么他可能被许多人认为是大帅哥。\n",
            "\n",
            "3. 着装：Andre的穿着打扮也会影响人们对他的看法。如果他的穿着时尚，符合他的年龄、身材和个性，那么他可能被许多人认为是大帅哥。\n",
            "\n",
            "4. 表情和态度：Andre的表情和态度也会影响人们对他的看法。如果他自信、友好、有礼貌，那么他可能被许多人认为是大帅哥。\n",
            "\n",
            "5. 社交网络影响力：如果Andre在社交媒体上有大量的粉丝和关注者，那么他可能被许多人认为是大帅哥。\n",
            "\n",
            "因此，要回答这个问题，需要考虑上述的所有因素，并结合不同人的看法和评价。\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 使用外部知识库\n",
        "!pip install faiss-gpu\n",
        "!pip install -U langchain-community\n",
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "id": "817KhFRQqDKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# 指定 Hugging Face 的模型名称\n",
        "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "\n",
        "# 创建 Hugging Face 嵌入对象\n",
        "embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oeX1iLFsuyn",
        "outputId": "85fc3af4-f8ea-444e-b1a6-1284e6ceccef"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-a922630eae15>:9: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "knowledges=[\"Andre是大帅哥\"]\n",
        "vectorstore = FAISS.from_texts(\n",
        "    knowledges, embedding=embeddings\n",
        ")\n",
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "LT2p5JMKsyL9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever.invoke(\"Andre\")[0].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "87tidzG8sz8f",
        "outputId": "5cbf476a-44d0-4ee6-e0df-289ff49a133d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Andre是大帅哥'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = ''\n",
        "sampling_params = SamplingParams(temperature=0.8,top_k=10, top_p=0.95,max_tokens=256,stop=[\"<|endoftext|>\",\"<|im_end|>\"])\n",
        "def generateText(prompts):\n",
        "    prompt = prompts.pop()\n",
        "    print(\"prompt:\",prompt)\n",
        "    background = retriever.invoke(prompt)[0].page_content\n",
        "    print(\"background:\",background)\n",
        "\n",
        "    input = str(f'''\\\n",
        "    {system_prompt+\"背景知识:\"+background+\"。用户的问题是:\" + prompt}''')\n",
        "\n",
        "    print(input)\n",
        "    output = llm.generate(input, sampling_params)\n",
        "    generated_text = output[0].outputs[0].text\n",
        "    generated_text=generated_text.replace(\"<|im_start|>\",\"\").replace(\"<|im_end|>\",\"\")\n",
        "    print(generated_text)\n",
        "    return output\n",
        "    # ret = {\"text\": str(generated_text)}\n",
        ""
      ],
      "metadata": {
        "id": "gd6iSy70vLEd"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ez6Y3bMcynkm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}